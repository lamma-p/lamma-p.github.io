<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-FSSR6942QL"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-FSSR6942QL');
    </script>

    <title>LaMMA-P: Generalizable Multi-Agent Long-Horizon Task Allocation and Planning with LM-Driven PDDL Planner</title>
    <link rel="icon" type="image/png" href="img/favicon_2.png">

    <meta name="description" content="LaMMA-P: Generalizable Multi-Agent Long-Horizon Task Allocation and Planning with LM-Driven PDDL Planner">
    <meta name="viewport" content="width=device-width, initial-scale=1">

<!--     <link rel="apple-touch-icon" href="apple-touch-icon.png"> -->
  <!-- <link rel="icon" type="image/png" href="img/seal_icon.png"> -->

    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.6.0/styles/default.min.css">
    <link rel="stylesheet" href="css/app.css">
    <!-- <link rel="stylesheet" href="css/bulma.min.css"> -->
    <!-- <link rel="stylesheet" href="css/bulma-carousel.min.css"> -->
    <!-- <link rel="stylesheet" href="css/bulma-slider.min.css"> -->
    <link rel="stylesheet" href="css/bulma-myscope.css">

    <!-- https://github.com/jgthms/bulma/issues/302#issuecomment-441890938 Follow this to use bulma style here without conflicts with app.css. I put bulma.min.css, bulma-carousel.min.css, and bulma-slider.min.css into a single file before applying less. -->

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.6.0/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    <script src="js/bulma-carousel.min.js"></script>
    <script src="js/bulma-slider.min.js"></script>
    <script src="js/index.js"></script>
    <script src="js/app.js"></script>

    <style>
        .nav-pills {
          position: relative;
          display: inline;
        }
        .imtip {
          position: absolute;
          top: 0;
          left: 0;
        }
    </style>
</head>
<body>
    <div class="container" id="main">
        <div class="row mt-4">
            <h2 class="col-md-12 text-center">
                <span style="font-weight: 900;">SoNIC</span>: </br> Safe <span style="font-weight: 900;">So</span>cial <span style="font-weight: 900;">N</span>avigation with Adaptive Conformal <span style="font-weight: 900;">I</span>nference and <span style="font-weight: 900;">C</span>onstrained Reinforcement Learning</br>
            </h2>
        </div>

        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                <!-- <br> -->
                <li><a href="//jyao97.github.io">Jianpeng Yao</a></li>
                <li><a href="//xiaopanz.github.io/">Xiaopan Zhang</a></li>
                <li>Yu Xia</li>
                <li>Zejing Wang</li>
                <li><a href="https://vcg.ece.ucr.edu/amit">Amit K. Roy-Chowdhury</a></li>
                <li><a href="https://jiachenli94.github.io/">Jiachen Li</a></li>
                <!-- <br> -->
                <br>
                <br>
                    <a href="https://robotics.ucr.edu//">
                        <image src="img/UCR_logo.svg" height="50px"> 
                    </a>
                </ul>
            </div>
        </div>
        
        <div class="row justify-content-md-center">
            <div class="col-md-2 text-center">
                <!-- <a href="https://arxiv.org/pdf/2407.17460"> -->
                    <img src="img/paper_img.png" height="45px">
                    <h4><strong>Paper (coming soon)</strong></h4>
                </a>
            </div>
            <div class="col-md-2 text-center">
                <!-- <a href="https://www.youtube.com/watch?v=uXqPMaSyDek"> -->
                    <img src="img/youtube_icon.png" height="45px">
                    <h4><strong>Video (coming soon)</strong></h4>
                </a>
            </div>
            <div class="col-md-2 text-center">
                <!-- <a href="https://github.com/tasl-lab/SoNIC/tree/main"> -->
                    <img src="img/github.png" height="45px">
                    <h4><strong>Code (coming soon)</strong></h4>
                </a>
            </div>
        </div>

        <div class="row justify-content-md-center">
            <div class="col-md-12 col-lg-10">
                <br>
                <h5 class="text-justify text-center">
                    <strong>La</strong>nguage <strong>M</strong>odel-Driven <strong>M</strong>ulti-<strong>A</strong>gent <strong>P</strong>DDL Planner, which integrates the structured problem-solving approach of the Planning Domain Definition Language (PDDL) with the strong reasoning ability of large language models (LLMs) to facilitate long-horizon task allocation and execution in heterogeneous multi-robot systems. 
                </h5>
                <br>
            </div>
        </div>

        <div class='myscope'>
          <section class="hero is-light is-small">
            <div class="hero-body">
              <div class="container">
                <div id="results-carousel" class="carousel results-carousel">
                  <!-- <div class="item-0">
                    <video poster="" id="0" autoplay controls muted loop playsinline height="100%">
                        <source src="videos/SoNIC_in_dist.mp4" type="video/mp4">
                    </video>
                  </div>
                  <div class="item-1">
                    <video poster="" id="1" autoplay controls muted loop playsinline height="100%">
                        <source src="videos/SoNIC_with_rushing_0.mp4" type="video/mp4">
                    </video>
                  </div>
                  <div class="item-2">
                    <video poster="" id="2" autoplay controls muted loop playsinline height="100%">
                        <source src="videos/SoNIC_with_rushing_1.mp4" type="video/mp4">
                    </video>
                  </div>
                  <div class="item-3">
                    <video poster="" id="4" autoplay controls muted loop playsinline height="100%">
                        <source src="videos/SoNIC_with_SF.mp4" type="video/mp4">
                    </video>
                  </div> -->
                </div>
              </div>
            </div>
          </section>
        </div>
        <br>
        <div class="row justify-content-md-center">
            <div class="col-md-12 col-lg-10">
                <h5 class="text-justify text-center">
                    Viusalizations LaMMA-P's Generated Plan in Different Scenarios
                </h5>
            </div>
        </div>
        <br>

        <div class="row justify-content-md-center">
            <div class="col-md-12 col-lg-10">
                <h3 class="mt-4 mb-2">
                    Abstract
                </h3>
                <p class="text-justify">
                    Language models (LMs) possess a strong capability to comprehend natural language, making them effective in translating human instructions into detailed plans for simple robot tasks. Nevertheless, it remains a significant challenge to handle long-horizon tasks, especially in subtask identification and allocation for cooperative heterogeneous robot teams. To address this issue, we propose a Language Model-Driven Multi-Agent PDDL Planner (LaMMA-P), a novel multi-agent task planning framework that achieves state-of-the-art performance on long-horizon tasks. LaMMA-P integrates the strengths of the LMs‚Äô reasoning capability and the traditional heuristic search planner to achieve a high success rate and efficiency while demonstrating strong generalization across tasks. Additionally, we create MAT-THOR, a comprehensive benchmark that features household tasks with two different levels of complexity based on the AI2-THOR environment. The experimental results demonstrate that LaMMA-P achieves a 105% higher success rate and 36% higher efficiency than existing LM-based multi-agent planners. 
                </p>
            </div>
        </div>
        
        <!-- TODO -->
        <!-- <div class="row justify-content-md-center">
            <div class="col-md-6 text-center">
                <iframe width="560" height="285" src="" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
            </div>
        </div> -->

        <!-- <div class="row justify-content-md-center">
            <blockquote class="twitter-tweet"><p lang="en" dir="ltr">LLMs can generate plans and write robot code üìù but they can also make mistakes. How do we get LLMs to ùò¨ùòØùò∞ùò∏ ùò∏ùò©ùò¶ùòØ ùòµùò©ùò¶ùò∫ ùò•ùò∞ùòØ&#39;ùòµ ùò¨ùòØùò∞ùò∏ ü§∑ and ask for help?<br><br>Read more on how we can do this (with statistical guarantees) for LLMs on robots üëá<a href="https://t.co/D7mHGzNP3p">https://t.co/D7mHGzNP3p</a> <a href="https://t.co/M9lUqlZ5cB">pic.twitter.com/M9lUqlZ5cB</a></p>&mdash; Allen Z. Ren (@allenzren) <a href="https://twitter.com/allenzren/status/1677000811803443213?ref_src=twsrc%5Etfw">July 6, 2023</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
        </div> -->

        <div class="row justify-content-md-center">
            <div class="col-md-12 col-lg-10">
                <br>
                <h3>
                    Key Ideas and Contributions
                </h3>
                <p class="text-justify">
                    <br><p style="text-align:center;">
                        <image src="img/overview.svg" width="100%">
                    </p>
                    <strong>1) Framework with PDDL and LLMs</strong>: We introduce a novel framework that integrates the reasoning ability of large language models (LLMs) with the heuristic planning algorithms of PDDL planners to address long-horizon task planning for heterogeneous robot teams. <br>
                    <strong>2) Modular Design</strong>: We develop a modular design that allows seamless integration of LLMs, PDDL planning systems, and simulation environments, which enables flexible task decomposition and the efficient allocation of sub-tasks based on the skills and capabilities of each robot. <br>
                    <strong>3) Novel Dataset and Performance Boost</strong>: We create MAT-THOR, a benchmark of multi-agent complex long-horizon tasks based on the AI2-THOR simulator, which evaluates the effectiveness and robustness of multi-agent planning methods by providing a standardized set of tasks and performance metrics for long-horizon task execution. Our method achieves state-of-the-art (SOTA) performance on this benchmark in terms of success rate and efficiency.<br>
            </div>
            <br>
        </div>
        <br>
        <div class="row justify-content-md-center">
            <div class="col-md-12 col-lg-10">
                <br>
                <h3>
                    Evaluation of LaMMA-P and baselines on different categories of tasks in the MAT-THOR dataset
                </h3>
                <p class="text-justify">
                    <br><p style="text-align:center;">
                        <image src="img/tables_lammap_1.png" width="100%">
                    </p>
                    <strong>Quantitative Analysis</strong>: In this paper, we validated LaMMA-P in three categories of tasks: Compound Tasks, Complex Tasks, and Vague Command Tasks. The experimental results demonstrate that LaMMA-P achieves SOTA performance in both success rate and efficiency compared to the strongest baseline SMART-LLM. Please refer to the paper for more details. <br>
            </div>
            <br>
        </div>
        <br>

<!--         <div class="row justify-content-md-center">
            <div class="col-md-12 col-lg-10">
                <h3>
                    Citation 
                </h3>
                <!-- TODO -->
                <!-- <a href="">[arxiv version]</a> -->
                <div class="form-group col-md-12">
                    <textarea id="bibtex" class="form-control" rows="6" readonly>
                        @article{yao2024sonic,
                            title={SoNIC: Safe Social Navigation with Adaptive Conformal Inference and Constrained Reinforcement Learning},
                            author={Yao, Jianpeng and Zhang, Xiaopan and Xia, Yu and Roy-Chowdhury, Amit K and Li, Jiachen},
                            journal={arXiv preprint arXiv:2407.17460},
                            year={2024}
                        }</textarea>
                </div>
            </div>
        </div> -->
        <!-- TODO -->
        <!-- <div class="row justify-content-md-center mt-4">
            <div class="col-md-12 col-lg-10">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                </p>
            </div>
        </div> -->
    </div>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/js/bootstrap.bundle.min.js" integrity="sha384-MrcW6ZMFYlzcLA8Nl+NtUVF0sA7MsXsP1UyJoMp4YLEuNSfAP+JcXn/tWtIaxVXM" crossorigin="anonymous"></script>
    <script>hljs.highlightAll();</script>
</body>
</html>
